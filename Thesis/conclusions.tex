\chapter{Conclusions and future research}\label{conclusions}
In this thesis we compared multiple state-space dimensionality reduction methods for a model-free, value-based reinforcement learning (RL) agent, using two environments: Starcraft II MoveToBeacon and OpenAI Atari Pong. We tried to answer the research question: \textit{what is the effect of state-space dimensionality reduction on model-free value-based reinforcement learning in an environment using spatial grid-based observations, by using PCA, autoencoders or DeepMDPs}? To answer this, we answered the research sub-question: \textit{how do the training results of a double deep-Q-network reinforcement learning agent change when using PCA, autoencoders and DeepMDPs for state-space dimensionality reduction in Starcraft II MoveToBeacon and OpenAI Atari Pong}? 

The PCA component was trained on previously gathered observations. In both environments, the PCA was unable to retain spatial information, despite using between $96\%$ and $99\%$ of the variance of the original observation. After training it was used in an RL agent to project the observation to a latent space, used by the RL agent to train its policy. Due to the lack of spatial information, needed for both environments, the PCA agent was unable to train well. Thus, PCA seems only promising for environments using non-spatial observations, like in Curran et al. where their PCA agent worked better than a baseline agent \cite{mario}.

The DeepMDP used in Starcraft II was also unable to train well. This is due to its difficulty in balancing multiple loss calculations. The DeepMDP agent initially trained to a decent policy, but then dropped down to a worse policy, due to the loss balancing problem. This is a known problem for DeepMDPs \cite{deepmdp}.

Lastly, the autoencoder was able to learn a good latent representation of the observations in both environments, meaning it accurately captured the dynamics of the original state space in a smaller dimensionality. We used the autoencoder in two different agent types. The first is the pre-trained autoencoder that is trained on previously gathered observations, as with PCA. This agent outperformed the baseline agent in Starcraft II, reaching a better and more consistent policy in fewer episodes: its average return increased roughly $10\%$ using $400$ instead of $600$ episodes to converge. In Pong it trained slightly worse than the baseline agent (a $20\%$ performance decrease), but still found a good policy (scoring about $70\%$ compared to an optimal, scripted agent). Its lesser performance here is explained by detailed spatial information being lost in the latent space.

The second autoencoder agent is the online trained autoencoder agent. Here, the autoencoder is trained simultaneously with the RL agent; thus the autoencoder is trained while being used to project observations to a latent representation. These agents took a longer time to train to a good policy but eventually reached one similar to the pre-trained autoencoder agent. This can be explained by the RL agent initially training on bad latent representation due to a yet badly trained autoencoder.

This showed the promise of using autoencoders for state-space dimensionality reduction in model-free RL agents. This allows for training agents in high dimensional environments, by projecting the observations to a lower dimensional space through state representation learning. This may result in lowered training times, on less observations and episodes, using less computation cost. Such a reduction component can also be used in different RL agents running in the same environment, solving different problems using the same latent representation. This way the autoencoder only has to be trained once for the environment, before being beneficial for the training of different RL agents. It also showed the limit of using an autoencoder for state-space dimensionality reduction in environments where detailed spatial information is necessary for the RL agent, since some of this might get lost in an even slightly inaccurate latent representation from the autoencoder.

PCA and DeepMDP showed less promise in these environments. The PCA lost its spatial information, and may thus be more applicable for non-spatial environments. The DeepMDP had trouble balancing its loss and was therefore unreliable.\newline

\noindent\textbf{Future work}\newline
\noindent For future work we suggest several possibilities. Firstly, we could try to improve on the experiments in this thesis. In particular, we could try to use a more complex autoencoder architecture, that only downsamples its input to a lower dimensional space in later layers of its encoder network (instead of doing this in the first layer like in this research). This way spatial information might be retained better, possibly resulting in even better training results in environments like Pong where spatial details are important. Secondly, we might use environments with non-spatial information to be able to compare the reductions methods here, to see how well PCA and DeepMDPs perform. 

Furthermore, we could examine the advantages of dimensionality reduction, since in this thesis we only focused on the feasibility of such agents. Whereas in this thesis we kept each agent as similar as possible to the baseline agent, for future research we could try to optimize the dimensionality reduction agents to train on latent spaces and policy networks that are as small as possible without converging to sub-optimal policies. This way we could better see the training advantages of such agents, both in terms of number of episodes and in terms of real training time. We could also examine the possibility of re-using the same reduction component in the same environment solving different RL problems.