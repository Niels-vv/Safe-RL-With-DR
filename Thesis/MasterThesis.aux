\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{deeprl}
\citation{AE_2019}
\citation{rlfitting}
\citation{AE_2019}
\citation{AE_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{3}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem statement}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Research questions}{3}{section.1.2}\protected@file@percent }
\citation{ddqn}
\citation{pysc2}
\citation{pong}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Method, results and possible benefits}{4}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Paper structure}{5}{section.1.4}\protected@file@percent }
\citation{mario}
\citation{pca_curran}
\citation{pca_neural}
\citation{pca_bitzer}
\citation{pca_bitzer}
\citation{AE_2010}
\citation{ae_visual}
\citation{AE_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related work}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{relatedwork}{{2}{7}{Related work}{chapter.2}{}}
\citation{vae}
\citation{rl_vae}
\citation{es}
\citation{rl_vaetwo}
\citation{rl_carla}
\citation{carla}
\citation{AE_2019}
\citation{deepmdp}
\citation{grokking}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Preliminaries}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{preliminaries}{{3}{9}{Preliminaries}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Reinforcement learning}{9}{section.3.1}\protected@file@percent }
\newlabel{pl-rl}{{3.1}{9}{Reinforcement learning}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}General overview}{9}{subsection.3.1.1}\protected@file@percent }
\citation{grokking}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The cycle of interaction between the environment and an agent in reinforcement learning.\relax }}{10}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rl_cycle}{{3.1}{10}{The cycle of interaction between the environment and an agent in reinforcement learning.\relax }{figure.caption.2}{}}
\citation{grokking}
\citation{grokking}
\citation{grokking}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Example of a Markov decision process (MDP).\relax }}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:mdp}{{3.2}{11}{Example of a Markov decision process (MDP).\relax }{figure.caption.3}{}}
\newlabel{reward}{{3.1}{11}{General overview}{equation.3.1.1}{}}
\citation{grokking}
\citation{grokking}
\citation{grokking}
\citation{nn}
\citation{nn}
\citation{nn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Artificial neural networks}{13}{subsection.3.1.2}\protected@file@percent }
\newlabel{pl-nn}{{3.1.2}{13}{Artificial neural networks}{subsection.3.1.2}{}}
\citation{qlearning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A convolutional layer with a single filter. The filter convolves across the input image to produce one feature map. Adding more filters would results in a stack of feature maps (one per filter).\relax }}{14}{figure.caption.4}\protected@file@percent }
\newlabel{fig:cnn}{{3.3}{14}{A convolutional layer with a single filter. The filter convolves across the input image to produce one feature map. Adding more filters would results in a stack of feature maps (one per filter).\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of a fully connected feedforward artificial neural network. For one node, example weights and activation function are shown.\relax }}{14}{figure.caption.5}\protected@file@percent }
\newlabel{fig:nn}{{3.4}{14}{Example of a fully connected feedforward artificial neural network. For one node, example weights and activation function are shown.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Double deep-q-network}{14}{subsection.3.1.3}\protected@file@percent }
\newlabel{pl-dqn}{{3.1.3}{14}{Double deep-q-network}{subsection.3.1.3}{}}
\citation{dqn}
\citation{ddqn}
\citation{grokking}
\citation{grokking}
\citation{grokking}
\citation{AE_2019}
\citation{AE_2019}
\citation{CNN_computation}
\citation{AE_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An overview of how a DDQN RL agent works.\relax }}{16}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ddqn}{{3.5}{16}{An overview of how a DDQN RL agent works.\relax }{figure.caption.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DDQN training step \cite  [p.299]{grokking}.\relax }}{17}{algorithm.1}\protected@file@percent }
\newlabel{alg:ddqn}{{1}{17}{DDQN training step \cite [p.299]{grokking}.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}State-space dimensionality reduction}{17}{section.3.2}\protected@file@percent }
\newlabel{pl-dimensionality}{{3.2}{17}{State-space dimensionality reduction}{section.3.2}{}}
\citation{pca}
\citation{mario}
\citation{AE_general}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Principal Component Analysis}{18}{subsection.3.2.1}\protected@file@percent }
\newlabel{pl-pca}{{3.2.1}{18}{Principal Component Analysis}{subsection.3.2.1}{}}
\newlabel{eigenvectors}{{3.6}{18}{Principal Component Analysis}{equation.3.2.6}{}}
\citation{AE_general}
\citation{deepmdp}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Autoencoder}{19}{subsection.3.2.2}\protected@file@percent }
\newlabel{pl-ae}{{3.2.2}{19}{Autoencoder}{subsection.3.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The architecture of an autoencoder.\relax }}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:AE_architecture}{{3.6}{19}{The architecture of an autoencoder.\relax }{figure.caption.7}{}}
\newlabel{enc}{{3.7}{19}{Autoencoder}{equation.3.2.7}{}}
\newlabel{dec}{{3.8}{19}{Autoencoder}{equation.3.2.8}{}}
\newlabel{encdec}{{3.9}{19}{Autoencoder}{equation.3.2.9}{}}
\citation{deepmdp}
\citation{wgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}DeepMDP}{20}{subsection.3.2.3}\protected@file@percent }
\newlabel{pl-deepmdp}{{3.2.3}{20}{DeepMDP}{subsection.3.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces An overview of how a DeepMDP agent works.\relax }}{20}{figure.caption.8}\protected@file@percent }
\newlabel{fig:deepmdp_agent}{{3.7}{20}{An overview of how a DeepMDP agent works.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Research}{22}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{research}{{4}{22}{Research}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Methodology}{22}{section.4.1}\protected@file@percent }
\newlabel{research-method}{{4.1}{22}{Methodology}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Experiments}{22}{subsection.4.1.1}\protected@file@percent }
\newlabel{research-exp}{{4.1.1}{22}{Experiments}{subsection.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Overview of the general architecture of an RL agent using a state-space dimensionality reduction method. In our experiments, the learning algorithm used is DDQN.\relax }}{23}{figure.caption.9}\protected@file@percent }
\newlabel{fig:rl_cycle_dim}{{4.1}{23}{Overview of the general architecture of an RL agent using a state-space dimensionality reduction method. In our experiments, the learning algorithm used is DDQN.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Overview of the baseline agent, using DDQN.\relax }}{23}{figure.caption.10}\protected@file@percent }
\newlabel{fig:rl_cycle_base}{{4.2}{23}{Overview of the baseline agent, using DDQN.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Overview of the PCA agent.\relax }}{24}{figure.caption.11}\protected@file@percent }
\newlabel{fig:rl_cycle_pca}{{4.3}{24}{Overview of the PCA agent.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Overview of the pre-trained and online trained autoencoder agents.\relax }}{24}{figure.caption.12}\protected@file@percent }
\newlabel{fig:rl_cycle_ae}{{4.4}{24}{Overview of the pre-trained and online trained autoencoder agents.\relax }{figure.caption.12}{}}
\citation{blizzard}
\citation{pysc2}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Overview of the DeepMDP agent.\relax }}{25}{figure.caption.13}\protected@file@percent }
\newlabel{fig:rl_cycle_deepmdp}{{4.5}{25}{Overview of the DeepMDP agent.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Environment: Starcraft II}{26}{subsection.4.1.2}\protected@file@percent }
\newlabel{research-env-pysc2}{{4.1.2}{26}{Environment: Starcraft II}{subsection.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Screenshot of the minigame \emph  {MoveToBeacon} in \emph  {StarCraft II}.\relax }}{26}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pysc2_SS}{{4.6}{26}{Screenshot of the minigame \emph {MoveToBeacon} in \emph {StarCraft II}.\relax }{figure.caption.14}{}}
\@writefile{tdo}{\contentsline {todo}{Is this correct? Or are there actually perhaps only 3 features or something?}{26}{section*.15}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{19505330}{25879076}
\pgfsyspdfmark {pgfid4}{35561850}{25892532}
\pgfsyspdfmark {pgfid5}{37183866}{25647346}
\@writefile{tdo}{\contentsline {todo}{Is this a correct usage of state-space?}{26}{section*.16}\protected@file@percent }
\pgfsyspdfmark {pgfid6}{21775496}{19640046}
\pgfsyspdfmark {pgfid9}{35561850}{16378232}
\pgfsyspdfmark {pgfid10}{37183866}{16133046}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A state observation received by the RL agent, for the StarCraft II minigame MoveToBeacon. The yellow cells represent one beacon; the blue cell represents the army unit controlled by the player; all other cells are empty.\relax }}{27}{figure.caption.17}\protected@file@percent }
\newlabel{fig:state_example}{{4.7}{27}{A state observation received by the RL agent, for the StarCraft II minigame MoveToBeacon. The yellow cells represent one beacon; the blue cell represents the army unit controlled by the player; all other cells are empty.\relax }{figure.caption.17}{}}
\@writefile{tdo}{\contentsline {todo}{Again, is this the correct use of "features"?}{27}{section*.19}\protected@file@percent }
\pgfsyspdfmark {pgfid11}{19785661}{22600692}
\pgfsyspdfmark {pgfid14}{35561850}{22614148}
\pgfsyspdfmark {pgfid15}{37183866}{22368962}
\citation{transpose}
\citation{pong}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Environment: OpenAI Pong}{29}{subsection.4.1.3}\protected@file@percent }
\newlabel{research-env-pong}{{4.1.3}{29}{Environment: OpenAI Pong}{subsection.4.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces A screenshot from the OpenAI Gym Atari game Pong.\relax }}{29}{figure.caption.20}\protected@file@percent }
\newlabel{fig:pong-screen}{{4.8}{29}{A screenshot from the OpenAI Gym Atari game Pong.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces A single observation in OpenAI Gym's Pong: four consecutive stacked frames.\relax }}{29}{figure.caption.21}\protected@file@percent }
\newlabel{fig:pong-obs}{{4.9}{29}{A single observation in OpenAI Gym's Pong: four consecutive stacked frames.\relax }{figure.caption.21}{}}
\citation{tsne}
\citation{maxvsconv}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Remarks on method}{31}{subsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Results}{32}{section.4.2}\protected@file@percent }
\newlabel{research-results}{{4.2}{32}{Results}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Research results: Starcraft II}{32}{subsection.4.2.1}\protected@file@percent }
\newlabel{research-results-pysc2}{{4.2.1}{32}{Research results: Starcraft II}{subsection.4.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1.1}Results overview}{32}{subsubsection.4.2.1.1}\protected@file@percent }
\newlabel{fig:results-base}{{4.10a}{33}{Results for the baseline agent.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:results-base}{{a}{33}{Results for the baseline agent.\relax }{figure.caption.23}{}}
\newlabel{fig:results-pca}{{4.10b}{33}{Results for the PCA agent.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:results-pca}{{b}{33}{Results for the PCA agent.\relax }{figure.caption.23}{}}
\newlabel{fig:results-ae}{{4.10c}{33}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:results-ae}{{c}{33}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.23}{}}
\newlabel{fig:results-online-ae}{{4.10d}{33}{Results for the online trained autoencoder agent.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:results-online-ae}{{d}{33}{Results for the online trained autoencoder agent.\relax }{figure.caption.23}{}}
\newlabel{fig:results-deepmdp}{{4.10e}{33}{Results for the DeepMDP agent.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:results-deepmdp}{{e}{33}{Results for the DeepMDP agent.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Results of the different RL agents in Starcraft II.\relax }}{33}{figure.caption.23}\protected@file@percent }
\newlabel{fig:results-agents}{{4.10}{33}{Results of the different RL agents in Starcraft II.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1.2}Discussion}{34}{subsubsection.4.2.1.2}\protected@file@percent }
\newlabel{research-discussion-pysc2}{{4.2.1.2}{34}{Discussion}{subsubsection.4.2.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.1.2.1}PCA Agent: losing all spatial information}{34}{paragraph.4.2.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.1.2.2}Autoencoder agents analyses: outperforming baseline agent}{34}{paragraph.4.2.1.2.2}\protected@file@percent }
\newlabel{fig:pca-original}{{4.11a}{35}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.24}{}}
\newlabel{sub@fig:pca-original}{{a}{35}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.24}{}}
\newlabel{fig:pca-latent}{{4.11b}{35}{The latent representation given by the PCA transformation\relax }{figure.caption.24}{}}
\newlabel{sub@fig:pca-latent}{{b}{35}{The latent representation given by the PCA transformation\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Latent representation of a state observation using PCA in Starcraft II.\relax }}{35}{figure.caption.24}\protected@file@percent }
\newlabel{fig:pca-state}{{4.11}{35}{Latent representation of a state observation using PCA in Starcraft II.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Losses per 25 observations for training the autoencoder on $240.000$ state observations.\relax }}{35}{figure.caption.25}\protected@file@percent }
\newlabel{fig:ae-loss}{{4.12}{35}{Losses per 25 observations for training the autoencoder on $240.000$ state observations.\relax }{figure.caption.25}{}}
\newlabel{fig:ae-featuremap-original}{{4.13a}{36}{The original observation, i.e. the input for the autoencoder.\relax }{figure.caption.26}{}}
\newlabel{sub@fig:ae-featuremap-original}{{a}{36}{The original observation, i.e. the input for the autoencoder.\relax }{figure.caption.26}{}}
\newlabel{fig:ae-featuremap-layer2}{{4.13b}{36}{The latent representation of the autoencoder, i.e. the output of the encoder.\relax }{figure.caption.26}{}}
\newlabel{sub@fig:ae-featuremap-layer2}{{b}{36}{The latent representation of the autoencoder, i.e. the output of the encoder.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces The latent representation of the autoencoder.\relax }}{36}{figure.caption.26}\protected@file@percent }
\newlabel{fig:ae-featuremap}{{4.13}{36}{The latent representation of the autoencoder.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Process of creating a correlation matrix for the latent features with the original features, and creating the correlation matrix of a single latent feature with the original features.\relax }}{37}{figure.caption.27}\protected@file@percent }
\newlabel{fig:ae-corr-process}{{4.14}{37}{Process of creating a correlation matrix for the latent features with the original features, and creating the correlation matrix of a single latent feature with the original features.\relax }{figure.caption.27}{}}
\citation{deepmdp}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.1.2.3}DeepMDP agent: unable to balance multiple loss calculations}{38}{paragraph.4.2.1.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Correlation matrix for the autoencoder used in the pre-trained autoencoder agent, based on $60.000$ state observations. The x-axis contains the features of the reduced state observations, and the y-axis contains the features of the original observations.\relax }}{39}{figure.caption.28}\protected@file@percent }
\newlabel{fig:ae-corr}{{4.15}{39}{Correlation matrix for the autoencoder used in the pre-trained autoencoder agent, based on $60.000$ state observations. The x-axis contains the features of the reduced state observations, and the y-axis contains the features of the original observations.\relax }{figure.caption.28}{}}
\newlabel{fig:ae-latent-feature}{{4.16a}{40}{The latent feature that was used: at index $67$ of flattened grid data.\relax }{figure.caption.29}{}}
\newlabel{sub@fig:ae-latent-feature}{{a}{40}{The latent feature that was used: at index $67$ of flattened grid data.\relax }{figure.caption.29}{}}
\newlabel{fig:ae-latent-feature-corr-matrix}{{4.16b}{40}{Correlation matrix for latent feature $68$ with the original features.\relax }{figure.caption.29}{}}
\newlabel{sub@fig:ae-latent-feature-corr-matrix}{{b}{40}{Correlation matrix for latent feature $68$ with the original features.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces The correlation matrix for latent feature $68$ with the original features.\relax }}{40}{figure.caption.29}\protected@file@percent }
\newlabel{fig:latent-feature-corr}{{4.16}{40}{The correlation matrix for latent feature $68$ with the original features.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Research results: OpenAI Pong}{41}{subsection.4.2.2}\protected@file@percent }
\newlabel{research-results-pong}{{4.2.2}{41}{Research results: OpenAI Pong}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.1}Results overview}{41}{subsubsection.4.2.2.1}\protected@file@percent }
\newlabel{fig:results-base-pong}{{4.17a}{41}{Results for the baseline agent.\relax }{figure.caption.30}{}}
\newlabel{sub@fig:results-base-pong}{{a}{41}{Results for the baseline agent.\relax }{figure.caption.30}{}}
\newlabel{fig:results-pca-pong}{{4.17b}{41}{Results for the PCA agent.\relax }{figure.caption.30}{}}
\newlabel{sub@fig:results-pca-pong}{{b}{41}{Results for the PCA agent.\relax }{figure.caption.30}{}}
\newlabel{fig:results-ae-pong}{{4.17c}{41}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.30}{}}
\newlabel{sub@fig:results-ae-pong}{{c}{41}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.30}{}}
\newlabel{fig:results-online-ae-pong}{{4.17d}{41}{Results for the online trained autoencoder agent.\relax }{figure.caption.30}{}}
\newlabel{sub@fig:results-online-ae-pong}{{d}{41}{Results for the online trained autoencoder agent.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Results of the different RL agents in Pong.\relax }}{41}{figure.caption.30}\protected@file@percent }
\newlabel{fig:results-agents-pong}{{4.17}{41}{Results of the different RL agents in Pong.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.2}Discussion}{43}{subsubsection.4.2.2.2}\protected@file@percent }
\newlabel{research-discussion-pong}{{4.2.2.2}{43}{Discussion}{subsubsection.4.2.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.2.2.1}PCA agent: losing spatial information}{43}{paragraph.4.2.2.2.1}\protected@file@percent }
\newlabel{fig:pca-original-pong}{{4.18a}{43}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.31}{}}
\newlabel{sub@fig:pca-original-pong}{{a}{43}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.31}{}}
\newlabel{fig:pca-latent-pong}{{4.18b}{43}{The latent representation given by the PCA transformation\relax }{figure.caption.31}{}}
\newlabel{sub@fig:pca-latent-pong}{{b}{43}{The latent representation given by the PCA transformation\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Latent representation of a state observation using PCA in Pong.\relax }}{43}{figure.caption.31}\protected@file@percent }
\newlabel{fig:pca-state-pong}{{4.18}{43}{Latent representation of a state observation using PCA in Pong.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.2.2.2}Pre-trained autoencoder agent: slightly worse than the baseline agent}{43}{paragraph.4.2.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Correlation matrix for the autoencoder used in the pre-trained autoencoder agent in Pong, based on $40.000$ state observations. The x-axis contains the features of the reduced state observations, and the y-axis contains the features of the original observations.\relax }}{46}{figure.caption.32}\protected@file@percent }
\newlabel{fig:ae-corr-pong}{{4.19}{46}{Correlation matrix for the autoencoder used in the pre-trained autoencoder agent in Pong, based on $40.000$ state observations. The x-axis contains the features of the reduced state observations, and the y-axis contains the features of the original observations.\relax }{figure.caption.32}{}}
\newlabel{fig:ae-latent-feature-pong}{{4.20a}{47}{The latent feature that was used: at index $903$ of flattened grid data.\relax }{figure.caption.33}{}}
\newlabel{sub@fig:ae-latent-feature-pong}{{a}{47}{The latent feature that was used: at index $903$ of flattened grid data.\relax }{figure.caption.33}{}}
\newlabel{fig:ae-latent-feature-corr-matrix-pong}{{4.20b}{47}{The correlation matrix for latent feature $904$ with the original features.\relax }{figure.caption.33}{}}
\newlabel{sub@fig:ae-latent-feature-corr-matrix-pong}{{b}{47}{The correlation matrix for latent feature $904$ with the original features.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces The correlation matrix for latent feature $904$ with the original features.\relax }}{47}{figure.caption.33}\protected@file@percent }
\newlabel{fig:latent-feature-corr-pong}{{4.20}{47}{The correlation matrix for latent feature $904$ with the original features.\relax }{figure.caption.33}{}}
\newlabel{fig:ae-latent-feature-pong2}{{4.21a}{48}{The latent feature that was used: at index $917$ of flattened grid data.\relax }{figure.caption.34}{}}
\newlabel{sub@fig:ae-latent-feature-pong2}{{a}{48}{The latent feature that was used: at index $917$ of flattened grid data.\relax }{figure.caption.34}{}}
\newlabel{fig:ae-latent-feature-corr-matrix-pong2}{{4.21b}{48}{The correlation matrix for latent feature $918$ with the original features.\relax }{figure.caption.34}{}}
\newlabel{sub@fig:ae-latent-feature-corr-matrix-pong2}{{b}{48}{The correlation matrix for latent feature $918$ with the original features.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces The correlation matrix for latent feature $918$ with the original features.\relax }}{48}{figure.caption.34}\protected@file@percent }
\newlabel{fig:latent-feature-corr-pong2}{{4.21}{48}{The correlation matrix for latent feature $918$ with the original features.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Losses per frame for training three autoencoders in Pong. Autoencoder 1 trained on $340.000$ frames to a loss of $0.9$, autoencoder 2 trained on $25.000$ frames to a loss of $30.7$ and autoencoder 3 trained on $10.000$ frames to a loss of $86.2$.\relax }}{48}{figure.caption.35}\protected@file@percent }
\newlabel{fig:ae-loss-pong}{{4.22}{48}{Losses per frame for training three autoencoders in Pong. Autoencoder 1 trained on $340.000$ frames to a loss of $0.9$, autoencoder 2 trained on $25.000$ frames to a loss of $30.7$ and autoencoder 3 trained on $10.000$ frames to a loss of $86.2$.\relax }{figure.caption.35}{}}
\newlabel{fig:ae-state-original-pong}{{4.23a}{49}{The original observation frame, i.e. the input for the autoencoder.\relax }{figure.caption.36}{}}
\newlabel{sub@fig:ae-state-original-pong}{{a}{49}{The original observation frame, i.e. the input for the autoencoder.\relax }{figure.caption.36}{}}
\newlabel{fig:ae1-state-pong}{{4.23b}{49}{The output of the encoder of autoencoder 1.\relax }{figure.caption.36}{}}
\newlabel{sub@fig:ae1-state-pong}{{b}{49}{The output of the encoder of autoencoder 1.\relax }{figure.caption.36}{}}
\newlabel{fig:ae2-state-pong}{{4.23c}{49}{The output of the encoder of autoencoder 2.\relax }{figure.caption.36}{}}
\newlabel{sub@fig:ae2-state-pong}{{c}{49}{The output of the encoder of autoencoder 2.\relax }{figure.caption.36}{}}
\newlabel{fig:ae3-state-pong}{{4.23d}{49}{The output of the encoder of autoencoder 3.\relax }{figure.caption.36}{}}
\newlabel{sub@fig:ae3-state-pong}{{d}{49}{The output of the encoder of autoencoder 3.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Latent representations of three different autoencoders for an observation frame in Pong.\relax }}{49}{figure.caption.36}\protected@file@percent }
\newlabel{fig:ae-output-pong}{{4.23}{49}{Latent representations of three different autoencoders for an observation frame in Pong.\relax }{figure.caption.36}{}}
\newlabel{fig:ae1-results-pong}{{4.24a}{50}{Training results for RL agent using pre-trained autoencoder 1 in Pong.\relax }{figure.caption.37}{}}
\newlabel{sub@fig:ae1-results-pong}{{a}{50}{Training results for RL agent using pre-trained autoencoder 1 in Pong.\relax }{figure.caption.37}{}}
\newlabel{fig:ae2-results-pong}{{4.24b}{50}{Training results for RL agent using pre-trained autoencoder 2 in Pong.\relax }{figure.caption.37}{}}
\newlabel{sub@fig:ae2-results-pong}{{b}{50}{Training results for RL agent using pre-trained autoencoder 2 in Pong.\relax }{figure.caption.37}{}}
\newlabel{fig:ae3-results-pong}{{4.24c}{50}{Training results for RL agent using pre-trained autoencoder 3 in Pong.\relax }{figure.caption.37}{}}
\newlabel{sub@fig:ae3-results-pong}{{c}{50}{Training results for RL agent using pre-trained autoencoder 3 in Pong.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces Training results for three different pre-trained autoencoder agents in Pong.\relax }}{50}{figure.caption.37}\protected@file@percent }
\newlabel{fig:ae-results-pong}{{4.24}{50}{Training results for three different pre-trained autoencoder agents in Pong.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.2.2.3}Online trained autoencoder agent: equaling the baseline agent's policy}{51}{paragraph.4.2.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Latent representations of the pre-trained autoencoder and online trained autoencoder for an observation frame in Pong.\relax }}{52}{figure.caption.38}\protected@file@percent }
\newlabel{fig:online-ae-output-pong}{{4.25}{52}{Latent representations of the pre-trained autoencoder and online trained autoencoder for an observation frame in Pong.\relax }{figure.caption.38}{}}
\citation{mario}
\citation{deepmdp}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Results discussion}{53}{subsection.4.2.3}\protected@file@percent }
\newlabel{research-discussion}{{4.2.3}{53}{Results discussion}{subsection.4.2.3}{}}
\citation{mario}
\citation{deepmdp}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions and future research}{55}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusions}{{5}{55}{Conclusions and future research}{chapter.5}{}}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{es}{1}
\bibcite{pca_bitzer}{2}
\bibcite{blizzard}{3}
\bibcite{pong}{4}
\bibcite{pca_curran}{5}
\bibcite{mario}{6}
\bibcite{carla}{7}
\bibcite{transpose}{8}
\bibcite{ae_visual}{9}
\bibcite{deepmdp}{10}
\bibcite{nn}{11}
\bibcite{wgan}{12}
\bibcite{rl_vae}{13}
\bibcite{gelu}{14}
\bibcite{AE_general}{15}
\bibcite{tsne}{16}
\bibcite{batchnorm}{17}
\bibcite{CNN_computation}{18}
\bibcite{qlearning}{19}
\bibcite{pca}{20}
\bibcite{rl_carla}{21}
\bibcite{vae}{22}
\bibcite{adam}{23}
\bibcite{AE_2010}{24}
\bibcite{rl_vaetwo}{25}
\bibcite{deeprl}{26}
\bibcite{dqn}{27}
\bibcite{grokking}{28}
\bibcite{relu}{29}
\bibcite{l1}{30}
\bibcite{AE_2019}{31}
\bibcite{pca_neural}{32}
\bibcite{maxvsconv}{33}
\bibcite{ddqn}{34}
\bibcite{AE_2016}{35}
\bibcite{pysc2}{36}
\bibcite{rlfitting}{37}
\citation{transpose}
\citation{relu}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{61}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix}{{A}{61}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Starcraft II: RL agent architectures}{61}{section.A.1}\protected@file@percent }
\newlabel{appendix-agents}{{A.1}{61}{Starcraft II: RL agent architectures}{section.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Baseline agent}{61}{subsection.A.1.1}\protected@file@percent }
\newlabel{appendix-baseline}{{A.1.1}{61}{Baseline agent}{subsection.A.1.1}{}}
\citation{adam}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces The decay of the epsilon value per episode.\relax }}{62}{figure.caption.40}\protected@file@percent }
\newlabel{fig:epsilon}{{A.1}{62}{The decay of the epsilon value per episode.\relax }{figure.caption.40}{}}
\citation{gelu}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}PCA agent}{63}{subsection.A.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.3}Pre-trained and online trained autoencoder agent}{63}{subsection.A.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.4}DeepMDP agent}{63}{subsection.A.1.4}\protected@file@percent }
\citation{l1}
\citation{wgan}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}OpenAI Pong: RL agent architectures}{64}{section.A.2}\protected@file@percent }
\newlabel{appendix-agents-pong}{{A.2}{64}{OpenAI Pong: RL agent architectures}{section.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Baseline agent}{64}{subsection.A.2.1}\protected@file@percent }
\newlabel{appendix-baseline-pong}{{A.2.1}{64}{Baseline agent}{subsection.A.2.1}{}}
\citation{batchnorm}
\citation{relu}
\citation{adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}PCA agent}{65}{subsection.A.2.2}\protected@file@percent }
\citation{gelu}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces The decay of the epsilon value per step in Pong.\relax }}{66}{figure.caption.41}\protected@file@percent }
\newlabel{fig:epsilon-pong}{{A.2}{66}{The decay of the epsilon value per step in Pong.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.3}Pre-trained and online trained autoencoder agent}{66}{subsection.A.2.3}\protected@file@percent }
