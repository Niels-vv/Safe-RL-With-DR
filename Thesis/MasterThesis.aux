\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{3}{Introduction}{chapter.1}{}}
\citation{grokking}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{preliminaries}{{2}{4}{Preliminaries}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Reinforcement learning}{4}{section.2.1}\protected@file@percent }
\newlabel{pl-rl}{{2.1}{4}{Reinforcement learning}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}General overview}{4}{subsection.2.1.1}\protected@file@percent }
\citation{grokking}
\citation{grokking}
\citation{grokking}
\citation{grokking}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The cycle of interaction between the environment and an agent in reinforcement learning.\relax }}{6}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rl_cycle}{{2.1}{6}{The cycle of interaction between the environment and an agent in reinforcement learning.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of a Markov decision process (MDP).\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:mdp}{{2.2}{6}{Example of a Markov decision process (MDP).\relax }{figure.caption.3}{}}
\newlabel{reward}{{2.1}{6}{General overview}{equation.2.1.1}{}}
\citation{grokking}
\citation{grokking}
\citation{grokking}
\citation{nn}
\citation{nn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Artificial neural networks}{8}{subsection.2.1.2}\protected@file@percent }
\newlabel{pl-nn}{{2.1.2}{8}{Artificial neural networks}{subsection.2.1.2}{}}
\citation{nn}
\citation{qlearning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A convolutional layer with a single filter. The filter convolves across the input image to produce one feature map. Adding more filters would results in a stack of feature maps (one per filter).\relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:cnn}{{2.3}{9}{A convolutional layer with a single filter. The filter convolves across the input image to produce one feature map. Adding more filters would results in a stack of feature maps (one per filter).\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of a fully connected feedforward artificial neural network. For one node, example weights and activation function are shown.\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:nn}{{2.4}{9}{Example of a fully connected feedforward artificial neural network. For one node, example weights and activation function are shown.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Double deep-q-network}{9}{subsection.2.1.3}\protected@file@percent }
\newlabel{pl-dqn}{{2.1.3}{9}{Double deep-q-network}{subsection.2.1.3}{}}
\citation{dqn}
\citation{ddqn}
\citation{grokking}
\citation{grokking}
\citation{grokking}
\citation{AE-2019}
\citation{AE_2019}
\citation{CNN_computation}
\citation{AE_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces An overview of how a DDQN RL agent works.\relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ddqn}{{2.5}{12}{An overview of how a DDQN RL agent works.\relax }{figure.caption.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DDQN training step \cite  [p.299]{grokking}.\relax }}{12}{algorithm.1}\protected@file@percent }
\newlabel{alg:ddqn}{{1}{12}{DDQN training step \cite [p.299]{grokking}.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}State-space dimensionality reduction}{12}{section.2.2}\protected@file@percent }
\newlabel{pl-dimensionality}{{2.2}{12}{State-space dimensionality reduction}{section.2.2}{}}
\citation{pca}
\citation{mario}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Principal Component Analysis}{13}{subsection.2.2.1}\protected@file@percent }
\newlabel{pl-pca}{{2.2.1}{13}{Principal Component Analysis}{subsection.2.2.1}{}}
\newlabel{eigenvectors}{{2.6}{13}{Principal Component Analysis}{equation.2.2.6}{}}
\citation{AE_general}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Autoencoder}{14}{subsection.2.2.2}\protected@file@percent }
\newlabel{pl-ae}{{2.2.2}{14}{Autoencoder}{subsection.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The architecture of an autoencoder.\relax }}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:AE_architecture}{{2.6}{14}{The architecture of an autoencoder.\relax }{figure.caption.7}{}}
\newlabel{enc}{{2.7}{14}{Autoencoder}{equation.2.2.7}{}}
\citation{AE_general}
\newlabel{dec}{{2.8}{15}{Autoencoder}{equation.2.2.8}{}}
\newlabel{encdec}{{2.9}{15}{Autoencoder}{equation.2.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}DeepMDP}{15}{subsection.2.2.3}\protected@file@percent }
\newlabel{pl-deepmdp}{{2.2.3}{15}{DeepMDP}{subsection.2.2.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Research}{17}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{research}{{3}{17}{Research}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Method}{17}{section.3.1}\protected@file@percent }
\newlabel{research-method}{{3.1}{17}{Method}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Experiments}{17}{subsection.3.1.1}\protected@file@percent }
\newlabel{research-exp}{{3.1.1}{17}{Experiments}{subsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of the general architecture of an RL agent using a state-space dimensionality reduction method. In our experiments, the learning algorithm used is DDQN.\relax }}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:rl_cycle_dim}{{3.1}{18}{Overview of the general architecture of an RL agent using a state-space dimensionality reduction method. In our experiments, the learning algorithm used is DDQN.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Overview of the baseline agent, using DDQN.\relax }}{19}{figure.caption.9}\protected@file@percent }
\newlabel{fig:rl_cycle_base}{{3.2}{19}{Overview of the baseline agent, using DDQN.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Overview of PCA agent.\relax }}{20}{figure.caption.10}\protected@file@percent }
\newlabel{fig:rl_cycle_pca}{{3.3}{20}{Overview of PCA agent.\relax }{figure.caption.10}{}}
\@writefile{tdo}{\contentsline {todo}{I guess "online" is not strictly correct}{20}{section*.12}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{7750942}{17808233}
\pgfsyspdfmark {pgfid4}{33621278}{17814214}
\pgfsyspdfmark {pgfid5}{35243294}{17569028}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Overview of the pre-trained and online trained autoencoder agents.\relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{fig:rl_cycle_ae}{{3.4}{21}{Overview of the pre-trained and online trained autoencoder agents.\relax }{figure.caption.11}{}}
\citation{blizzard}
\citation{pysc2}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Overview of the DeepMDP agent.\relax }}{22}{figure.caption.13}\protected@file@percent }
\newlabel{fig:rl_cycle_deepmdp}{{3.5}{22}{Overview of the DeepMDP agent.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Environment: Starcraft II}{22}{subsection.3.1.2}\protected@file@percent }
\newlabel{research-env-pysc2}{{3.1.2}{22}{Environment: Starcraft II}{subsection.3.1.2}{}}
\@writefile{tdo}{\contentsline {todo}{Is this correct? Or are there actually perhaps only 3 features or something?}{22}{section*.15}\protected@file@percent }
\pgfsyspdfmark {pgfid6}{24729786}{21395581}
\pgfsyspdfmark {pgfid9}{33621278}{21409037}
\pgfsyspdfmark {pgfid10}{35243294}{21163851}
\@writefile{tdo}{\contentsline {todo}{Is this a correct usage of state-space?}{22}{section*.16}\protected@file@percent }
\pgfsyspdfmark {pgfid11}{24749795}{14265261}
\pgfsyspdfmark {pgfid14}{33621278}{11894737}
\pgfsyspdfmark {pgfid15}{35243294}{11649551}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Screenshot of the minigame \emph  {MoveToBeacon} in \emph  {StarCraft II}.\relax }}{23}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pysc2_SS}{{3.6}{23}{Screenshot of the minigame \emph {MoveToBeacon} in \emph {StarCraft II}.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Agents setup}{23}{section*.18}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Again, is this correct?}{23}{section*.19}\protected@file@percent }
\pgfsyspdfmark {pgfid16}{29908662}{11312494}
\pgfsyspdfmark {pgfid19}{33621278}{11325950}
\pgfsyspdfmark {pgfid20}{35243294}{11080764}
\citation{transpose}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A state observation received by the RL agent, for the StarCraft II minigame MoveToBeacon. The yellow cells represent one beacon; the blue cell represents the army unit controlled by the player; all other cells are empty.\relax }}{24}{figure.caption.17}\protected@file@percent }
\newlabel{fig:state_example}{{3.7}{24}{A state observation received by the RL agent, for the StarCraft II minigame MoveToBeacon. The yellow cells represent one beacon; the blue cell represents the army unit controlled by the player; all other cells are empty.\relax }{figure.caption.17}{}}
\@writefile{tdo}{\contentsline {todo}{I guess "online" is not strictly correct}{25}{section*.20}\protected@file@percent }
\pgfsyspdfmark {pgfid21}{7750942}{18220753}
\pgfsyspdfmark {pgfid24}{33621278}{18226734}
\pgfsyspdfmark {pgfid25}{35243294}{17981548}
\citation{pong}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Environment: OpenAI Pong}{26}{subsection.3.1.3}\protected@file@percent }
\newlabel{research-env-pong}{{3.1.3}{26}{Environment: OpenAI Pong}{subsection.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces A screenshot from the OpenAI Gym Atari game Pong.\relax }}{26}{figure.caption.21}\protected@file@percent }
\newlabel{fig:pong-screen}{{3.8}{26}{A screenshot from the OpenAI Gym Atari game Pong.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces A single observation in OpenAI Gym's Pong: four consecutive stacked frames.\relax }}{27}{figure.caption.22}\protected@file@percent }
\newlabel{fig:pong-obs}{{3.9}{27}{A single observation in OpenAI Gym's Pong: four consecutive stacked frames.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{Agents setup}{27}{section*.23}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{I guess "online" is not strictly correct}{28}{section*.24}\protected@file@percent }
\pgfsyspdfmark {pgfid26}{7750942}{11981723}
\pgfsyspdfmark {pgfid29}{33621278}{11987704}
\pgfsyspdfmark {pgfid30}{35243294}{11742518}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Results}{29}{section.3.2}\protected@file@percent }
\newlabel{research-results}{{3.2}{29}{Results}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Research results: Starcraft II}{29}{subsection.3.2.1}\protected@file@percent }
\newlabel{research-results-pysc2}{{3.2.1}{29}{Research results: Starcraft II}{subsection.3.2.1}{}}
\newlabel{fig:results-base}{{3.10a}{30}{Results for the baseline agent.\relax }{figure.caption.25}{}}
\newlabel{sub@fig:results-base}{{a}{30}{Results for the baseline agent.\relax }{figure.caption.25}{}}
\newlabel{fig:results-pca}{{3.10b}{30}{Results for the PCA agent.\relax }{figure.caption.25}{}}
\newlabel{sub@fig:results-pca}{{b}{30}{Results for the PCA agent.\relax }{figure.caption.25}{}}
\newlabel{fig:results-ae}{{3.10c}{30}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.25}{}}
\newlabel{sub@fig:results-ae}{{c}{30}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Results of the different RL agents in Starcraft II.\relax }}{30}{figure.caption.25}\protected@file@percent }
\newlabel{fig:results-online-ae}{{3.10d}{31}{Results for the online trained autoencoder agent.\relax }{figure.caption.26}{}}
\newlabel{sub@fig:results-online-ae}{{d}{31}{Results for the online trained autoencoder agent.\relax }{figure.caption.26}{}}
\newlabel{fig:results-deepmdp}{{3.10e}{31}{Results for the DeepMDP agent.\relax }{figure.caption.26}{}}
\newlabel{sub@fig:results-deepmdp}{{e}{31}{Results for the DeepMDP agent.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Results of the different RL agents in Starcraft II(cont.).\relax }}{31}{figure.caption.26}\protected@file@percent }
\newlabel{fig:results-agents}{{3.10}{31}{Results of the different RL agents in Starcraft II(cont.).\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{31}{section*.27}\protected@file@percent }
\newlabel{research-discussion-pysc2}{{3.2.1}{31}{Discussion}{section*.27}{}}
\@writefile{tdo}{\contentsline {todo}{Citing}{31}{section*.28}\protected@file@percent }
\pgfsyspdfmark {pgfid31}{30367142}{12056197}
\pgfsyspdfmark {pgfid34}{33621278}{12069653}
\pgfsyspdfmark {pgfid35}{35243294}{11824467}
\newlabel{fig:pca-original}{{3.11a}{33}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.29}{}}
\newlabel{sub@fig:pca-original}{{a}{33}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.29}{}}
\newlabel{fig:pca-latent}{{3.11b}{33}{The latent representation given by the PCA transformation\relax }{figure.caption.29}{}}
\newlabel{sub@fig:pca-latent}{{b}{33}{The latent representation given by the PCA transformation\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Latent representation a state observation using PCA.\relax }}{33}{figure.caption.29}\protected@file@percent }
\newlabel{fig:pca-state}{{3.11}{33}{Latent representation a state observation using PCA.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Losses per 25 observations for training the autoencoder on $240.000$ state observations.\relax }}{34}{figure.caption.30}\protected@file@percent }
\newlabel{fig:ae-loss}{{3.12}{34}{Losses per 25 observations for training the autoencoder on $240.000$ state observations.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Research results: OpenAI Pong}{37}{subsection.3.2.2}\protected@file@percent }
\newlabel{research-results-pong}{{3.2.2}{37}{Research results: OpenAI Pong}{subsection.3.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{37}{section*.37}\protected@file@percent }
\newlabel{research-discussion-pong}{{3.2.2}{37}{Discussion}{section*.37}{}}
\@writefile{tdo}{\contentsline {todo}{Citing, same as in pysc2}{37}{section*.38}\protected@file@percent }
\pgfsyspdfmark {pgfid36}{16762943}{14499611}
\pgfsyspdfmark {pgfid39}{33621278}{14513067}
\pgfsyspdfmark {pgfid40}{35243294}{14267881}
\citation{mario}
\citation{deepmdp}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Results discussion}{39}{subsection.3.2.3}\protected@file@percent }
\newlabel{research-discussion}{{3.2.3}{39}{Results discussion}{subsection.3.2.3}{}}
\newlabel{fig:ae-featuremap-original}{{3.13a}{41}{The original observation, i.e. the input for the autoencoder.\relax }{figure.caption.31}{}}
\newlabel{sub@fig:ae-featuremap-original}{{a}{41}{The original observation, i.e. the input for the autoencoder.\relax }{figure.caption.31}{}}
\newlabel{fig:ae-featuremap-layer2}{{3.13b}{41}{The latent representation of the autoencoder, i.e. the output of the encoder.\relax }{figure.caption.31}{}}
\newlabel{sub@fig:ae-featuremap-layer2}{{b}{41}{The latent representation of the autoencoder, i.e. the output of the encoder.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces The latent representation of the autoencoder.\relax }}{41}{figure.caption.31}\protected@file@percent }
\newlabel{fig:ae-featuremap}{{3.13}{41}{The latent representation of the autoencoder.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Process of creating a correlation matrix for the latent features with the original features, and creating the correlation matrix of a single latent feature with the original features.\relax }}{42}{figure.caption.32}\protected@file@percent }
\newlabel{fig:ae-corr-process}{{3.14}{42}{Process of creating a correlation matrix for the latent features with the original features, and creating the correlation matrix of a single latent feature with the original features.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Correlation matrix for the autoencoder used in the pre-trained autoencoder agent, based on $60.000$ state observations. The x-axis contains the features of the original observations, and the y-axis contains the features of the reduced state observations.\relax }}{42}{figure.caption.33}\protected@file@percent }
\newlabel{fig:ae-corr}{{3.15}{42}{Correlation matrix for the autoencoder used in the pre-trained autoencoder agent, based on $60.000$ state observations. The x-axis contains the features of the original observations, and the y-axis contains the features of the reduced state observations.\relax }{figure.caption.33}{}}
\newlabel{fig:ae-latent-feature}{{3.16a}{43}{The latent feature that was used.\relax }{figure.caption.34}{}}
\newlabel{sub@fig:ae-latent-feature}{{a}{43}{The latent feature that was used.\relax }{figure.caption.34}{}}
\newlabel{fig:ae-latent-feature-corr-matrix}{{3.16b}{43}{The feauture map of the first convoluational layer, showing the output of its $32$ channels.\relax }{figure.caption.34}{}}
\newlabel{sub@fig:ae-latent-feature-corr-matrix}{{b}{43}{The feauture map of the first convoluational layer, showing the output of its $32$ channels.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces The correlation matrix for latent feature with the original features.\relax }}{43}{figure.caption.34}\protected@file@percent }
\newlabel{fig:latent-feature-corr}{{3.16}{43}{The correlation matrix for latent feature with the original features.\relax }{figure.caption.34}{}}
\newlabel{fig:results-base-pong}{{3.17a}{44}{Results for the baseline agent.\relax }{figure.caption.35}{}}
\newlabel{sub@fig:results-base-pong}{{a}{44}{Results for the baseline agent.\relax }{figure.caption.35}{}}
\newlabel{fig:results-pca-pong}{{3.17b}{44}{Results for the PCA agent.\relax }{figure.caption.35}{}}
\newlabel{sub@fig:results-pca-pong}{{b}{44}{Results for the PCA agent.\relax }{figure.caption.35}{}}
\newlabel{fig:results-ae-pong}{{3.17c}{44}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.35}{}}
\newlabel{sub@fig:results-ae-pong}{{c}{44}{Results for the pre-trained autoencoder agent.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Results of the different RL agents in Pong.\relax }}{44}{figure.caption.35}\protected@file@percent }
\newlabel{fig:results-online-ae-pong}{{3.17d}{45}{Results for the online trained autoencoder agent.\relax }{figure.caption.36}{}}
\newlabel{sub@fig:results-online-ae-pong}{{d}{45}{Results for the online trained autoencoder agent.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Results of the different RL agents in Pong(cont.).\relax }}{45}{figure.caption.36}\protected@file@percent }
\newlabel{fig:results-agents-pong}{{3.17}{45}{Results of the different RL agents in Pong(cont.).\relax }{figure.caption.36}{}}
\newlabel{fig:pca-original-pong}{{3.18a}{45}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.39}{}}
\newlabel{sub@fig:pca-original-pong}{{a}{45}{The original observation, i.e. the input for the PCA transformation.\relax }{figure.caption.39}{}}
\newlabel{fig:pca-latent-pong}{{3.18b}{45}{The latent representation given by the PCA transformation\relax }{figure.caption.39}{}}
\newlabel{sub@fig:pca-latent-pong}{{b}{45}{The latent representation given by the PCA transformation\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Latent representation of a state observation using PCA.\relax }}{45}{figure.caption.39}\protected@file@percent }
\newlabel{fig:pca-state-pong}{{3.18}{45}{Latent representation of a state observation using PCA.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Losses per 25 observation frames for training three autoencoders in Pong. Autoencoder 1 trained on $332.000$ frames to a loss of $0.9$, autoencoder 2 trained on $25.000$ frames to a loss of $30.7$ and autoencoder 3 trained on $10.000$ frames to a loss of $86.2$.\relax }}{46}{figure.caption.40}\protected@file@percent }
\newlabel{fig:ae-loss-pong}{{3.19}{46}{Losses per 25 observation frames for training three autoencoders in Pong. Autoencoder 1 trained on $332.000$ frames to a loss of $0.9$, autoencoder 2 trained on $25.000$ frames to a loss of $30.7$ and autoencoder 3 trained on $10.000$ frames to a loss of $86.2$.\relax }{figure.caption.40}{}}
\newlabel{fig:ae-state-original-pong}{{3.20a}{47}{The original observation frame, i.e. the input for the autoencoder.\relax }{figure.caption.41}{}}
\newlabel{sub@fig:ae-state-original-pong}{{a}{47}{The original observation frame, i.e. the input for the autoencoder.\relax }{figure.caption.41}{}}
\newlabel{fig:ae1-state-pong}{{3.20b}{47}{The output of the encoder of autoencoder 1.\relax }{figure.caption.41}{}}
\newlabel{sub@fig:ae1-state-pong}{{b}{47}{The output of the encoder of autoencoder 1.\relax }{figure.caption.41}{}}
\newlabel{fig:ae2-state-pong}{{3.20c}{47}{The output of the encoder of autoencoder 2.\relax }{figure.caption.41}{}}
\newlabel{sub@fig:ae2-state-pong}{{c}{47}{The output of the encoder of autoencoder 2.\relax }{figure.caption.41}{}}
\newlabel{fig:ae3-state-pong}{{3.20d}{47}{The output of the encoder of autoencoder 3.\relax }{figure.caption.41}{}}
\newlabel{sub@fig:ae3-state-pong}{{d}{47}{The output of the encoder of autoencoder 3.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Latent representations of three different autoencoders for an observation frame in Pong.\relax }}{47}{figure.caption.41}\protected@file@percent }
\newlabel{fig:ae-output-pong}{{3.20}{47}{Latent representations of three different autoencoders for an observation frame in Pong.\relax }{figure.caption.41}{}}
\newlabel{fig:ae1-results-pong}{{3.21a}{48}{Training results for RL agent using pre-trained autoencoder 1 in Pong.\relax }{figure.caption.42}{}}
\newlabel{sub@fig:ae1-results-pong}{{a}{48}{Training results for RL agent using pre-trained autoencoder 1 in Pong.\relax }{figure.caption.42}{}}
\newlabel{fig:ae2-results-pong}{{3.21b}{48}{Training results for RL agent using pre-trained autoencoder 2 in Pong.\relax }{figure.caption.42}{}}
\newlabel{sub@fig:ae2-results-pong}{{b}{48}{Training results for RL agent using pre-trained autoencoder 2 in Pong.\relax }{figure.caption.42}{}}
\newlabel{fig:ae3-results-pong}{{3.21c}{48}{Training results for RL agent using pre-trained autoencoder 3 in Pong.\relax }{figure.caption.42}{}}
\newlabel{sub@fig:ae3-results-pong}{{c}{48}{Training results for RL agent using pre-trained autoencoder 3 in Pong.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Latent representations of three different autoencoders for an observation frame in Pong.\relax }}{48}{figure.caption.42}\protected@file@percent }
\newlabel{fig:ae-results-pong}{{3.21}{48}{Latent representations of three different autoencoders for an observation frame in Pong.\relax }{figure.caption.42}{}}
\citation{mario}
\citation{AE_2010}
\citation{AE_2016}
\citation{AE_2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Related work}{49}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{relatedwork}{{4}{49}{Related work}{chapter.4}{}}
\citation{deepmdp}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Future work}{51}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{futurework}{{5}{51}{Future work}{chapter.5}{}}
\bibstyle{plain}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{52}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusions}{{6}{52}{Conclusions}{chapter.6}{}}
\bibcite{blizzard}{1}
\bibcite{pong}{2}
\bibcite{mario}{3}
\bibcite{transpose}{4}
\bibcite{deepmdp}{5}
\bibcite{nn}{6}
\bibcite{wgan}{7}
\bibcite{gelu}{8}
\bibcite{AE_general}{9}
\bibcite{CNN_computation}{10}
\bibcite{qlearning}{11}
\bibcite{pca}{12}
\bibcite{adam}{13}
\bibcite{AE_2010}{14}
\bibcite{dqn}{15}
\bibcite{grokking}{16}
\bibcite{relu}{17}
\bibcite{l1}{18}
\bibcite{AE_2019}{19}
\bibcite{ddqn}{20}
\bibcite{AE_2016}{21}
\bibcite{pysc2}{22}
\citation{transpose}
\citation{relu}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{56}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix}{{A}{56}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Starcraft II: RL agent architectures}{56}{section.A.1}\protected@file@percent }
\newlabel{appendix-agents}{{A.1}{56}{Starcraft II: RL agent architectures}{section.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Baseline agent}{56}{subsection.A.1.1}\protected@file@percent }
\newlabel{appendix-baseline}{{A.1.1}{56}{Baseline agent}{subsection.A.1.1}{}}
\citation{adam}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces The decay of the epsilon value per episode.\relax }}{57}{figure.caption.44}\protected@file@percent }
\newlabel{fig:epsilon}{{A.1}{57}{The decay of the epsilon value per episode.\relax }{figure.caption.44}{}}
\citation{gelu}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}PCA agent}{58}{subsection.A.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.3}Pre-trained and online trained autoencoder agent}{58}{subsection.A.1.3}\protected@file@percent }
\citation{l1}
\citation{wgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.4}DeepMDP agent}{59}{subsection.A.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}OpenAI Pong: RL agent architectures}{59}{section.A.2}\protected@file@percent }
\newlabel{appendix-agents-pong}{{A.2}{59}{OpenAI Pong: RL agent architectures}{section.A.2}{}}
