\chapter{Related work}\label{relatedwork}
State-space dimensionality reduction has been a topic of interest in RL for several years. In their 2015 paper, Curran et al. \cite{mario} used PCA to reduce the dimensionality of the state-space in a Super Mario environment. They found that with the right number of principal components, an agent using PCA was able to converge to a better policy and needed less episodes than an agent using the full observation. Curran et al. corrobarated these results in more environments in later research in 2016 \cite{pca_curran}. Similar results were found by Shah et al. in 2011 in the context of neuroscience \cite{pca_neural}. Bitzer et al. also found similar results in 2010. \cite{pca_bitzer}. Reducing the dimensionality using PCA led to better policies and quicker convergences; this was interpreted as ``[...] due to the higher dimensionality, learning in the full 4D state space requires far more exploration to cover the same proportion of space" \cite{pca_bitzer}.

Unlike the Starcraft II environment used in our research having image/grid based observations, the states and observations used in these papers are comprised of non-spatial related variables, i.e. linear states; to, for instance, denote the presence of one or more enemies within one grid-cell distance of Mario's position, Curran et al. use one variable with $2^8$ possible values ($0$-$255$). Hence we are using PCA in a different setting, with an observation format more commonly used in modern RL. Furthermore they use the Q-learning algorithm (mentioned in section \ref{pl-dqn}) which is not a state-of-the-art learning algorithm anymore and handles larger state-spaces badly due to using a lookup-table instead of a neural network for the Q-function.

Research has also been done with regards to using autoencoders for state-space-dimensionality reduction in RL. Lange and Riedmiller in 2010 \cite{AE_2010} used visual data as observations and found that the agent was able to find an optimal policy using lower dimensional data from the autoencoder. They did not compare the performance of this agent with any other agent. Furthermore, their system only has $31$ possible states, which is very limited; in contrast, our Starcaft II environment has a total of $731.187$ possible states. 

In 2016, Finn et al. successfully showed the possibility of training an RL agent on a latent representation given by an autoencoder on visual information \cite{ae_visual}. The same year, Van Hoof et al. \cite{AE_2016} used an autoencoder to project noisy sensor data unto a lower dimensional space. They found that the agent using the autoencoder was far better able to find a good policy than the agent using full observations. This was explained as the agent being too sensitive to noisy data, hence being unable to train even decently. Our research in contrast explores an environment where the baseline agent is able to train to a good policy.

Ha and Schmidhuber use a variational autoencoder (VAE) \cite{vae} in their 2018 paper to train an agent in image based environments \cite{rl_vae}. Through this method, their agent outperformed baseline agents. They did find that the VAE produced mixed results for the latent representation; in an environment where details are important, they were lost in the latent representation. Differently from our research, they do not compare different reduction methods. Furthermore they do not use deep reinforcement learning, but train the agent using \textit{evolution strategies} \cite{es}. Lee et al. also train an agent on a variational autoencoder and found similar results \cite{rl_vaetwo}.

Khan et al. used an autoencoder to project visual data to a latent space and were able to train a self-driving car in the CARLA environment \cite{rl_carla}\cite{carla}. However, no comparison to a baseline agent (or other reduction methods) was given.

In 2019, Prakash et al. \cite{AE_2019} also used an autoencoder on visual data to project the observation data onto a lower dimensional space. They found that an agent using the autoencoder far outperformed the baseline agent. The baseline agent did not find a decent policy, though the authors claim that with enough episodes it would have.

Another paper in 2019 by Gelada et al. \cite{deepmdp} compared using a DeepMDP with using an autoencoder for state-space dimensionality reduction. They found that in simpler environments a DeepMDP can find better representations on lower dimensionality space than an autoencoder. Simultaneously though, they also find that in more complex environments, like Atari games, DeepMDPs can have trouble finding a good representation and that its loss can be difficult to optimize. They also find that a DeepMDP agent generally outperforms a baseline agent. 

%project_matrix_2020
%dimred_overview_2008


