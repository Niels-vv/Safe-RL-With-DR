\begin{abstract}
In this thesis we compare several state-space dimensionality reduction methods for model-free reinforcement learning (RL) agents. In state-space dimensionality reduction, observations from an environment are projected to a lower dimensional space, allowing for RL agents to use smaller artificial neural networks to train their policy. We use and compare the following reduction methods: principal component analysis (PCA), autoencoders, and DeepMDPs. The PCA is trained on gathered observations before being used by the RL agent. The autoencoder is used in two types of agents: one where the autoencoder is trained before being used by the RL agent, as with PCA, and one where the autoencoder is trained online, i.e. simultaneously with the RL agent. The training results of these agents and a baseline DDQN agent were compared using two environments using spatial observations: Starcraft II MoveToBeacon and OpenAI Atari Pong. Apart from the baseline agents, only the autoencoder agents were able to train to good policies; in Starcraft II, the pre-trained autoencoder agent learned a better policy in fewer episodes than the baseline agent.
\end{abstract}
