\begin{abstract}
In this thesis, we compare several state-space dimensionality reduction methods for model-free reinforcement learning (RL) agents. In state-space dimensionality reduction, observations from an environment are projected to a lower dimensional space, allowing RL agents to use smaller artificial neural networks to train their policy. We will look at the effect of using state-space dimensionality reduction on the training process and results of RL agents. We use and compare the following reduction methods: principal component analysis (PCA), autoencoders, and DeepMDPs. The PCA component is trained on gathered observations before being used by the RL agent. The autoencoder is used in two types of agents: one where the autoencoder is trained before being used by the RL agent, as with PCA, and one where the autoencoder is trained online, i.e. simultaneously with the RL agent. The training results of these agents and a baseline agent, all using the double deep-q-network (DDQN) learning algorithm, were compared using two environments using spatial observations: Starcraft II MoveToBeacon and OpenAI Atari Pong. Apart from the baseline agents, only the autoencoder agents were able to train to good policies (scoring $80-85\%$ of an optimal, scripted agent's score per episode); in Starcraft II, the pre-trained autoencoder agent learned a better policy in fewer episodes than the baseline agent (a $10\%$ improvement while converging in $400$ instead of $600$ episodes) and in Pong it learned a policy slightly worse than the baseline agent (a $20\%$ decrease, converging in $350$ instead of $250$ episodes).
\end{abstract}
