\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Problem statement}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Research questions}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Method, results and possible benefits}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{Paper structure}{chapter.1}% 5
\BOOKMARK [0][-]{chapter.2}{Preliminaries}{}% 6
\BOOKMARK [1][-]{section.2.1}{Reinforcement learning}{chapter.2}% 7
\BOOKMARK [2][-]{subsection.2.1.1}{General overview}{section.2.1}% 8
\BOOKMARK [2][-]{subsection.2.1.2}{Artificial neural networks}{section.2.1}% 9
\BOOKMARK [2][-]{subsection.2.1.3}{Double deep-q-network}{section.2.1}% 10
\BOOKMARK [1][-]{section.2.2}{State-space dimensionality reduction}{chapter.2}% 11
\BOOKMARK [2][-]{subsection.2.2.1}{Principal Component Analysis}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.2}{Autoencoder}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.3}{DeepMDP}{section.2.2}% 14
\BOOKMARK [0][-]{chapter.3}{Research}{}% 15
\BOOKMARK [1][-]{section.3.1}{Methodology}{chapter.3}% 16
\BOOKMARK [2][-]{subsection.3.1.1}{Experiments}{section.3.1}% 17
\BOOKMARK [2][-]{subsection.3.1.2}{Environment: Starcraft II}{section.3.1}% 18
\BOOKMARK [3][-]{subsubsection.3.1.2.1}{Agents setup}{subsection.3.1.2}% 19
\BOOKMARK [2][-]{subsection.3.1.3}{Environment: OpenAI Pong}{section.3.1}% 20
\BOOKMARK [3][-]{subsubsection.3.1.3.1}{Agents setup}{subsection.3.1.3}% 21
\BOOKMARK [2][-]{subsection.3.1.4}{Remarks on method}{section.3.1}% 22
\BOOKMARK [1][-]{section.3.2}{Results}{chapter.3}% 23
\BOOKMARK [2][-]{subsection.3.2.1}{Research results: Starcraft II}{section.3.2}% 24
\BOOKMARK [3][-]{subsubsection.3.2.1.1}{Discussion}{subsection.3.2.1}% 25
\BOOKMARK [4][-]{paragraph.3.2.1.1.1}{PCA Agent: losing all spatial information}{subsubsection.3.2.1.1}% 26
\BOOKMARK [4][-]{paragraph.3.2.1.1.2}{Autoencoder agents analyses: outperforming baseline agent}{subsubsection.3.2.1.1}% 27
\BOOKMARK [4][-]{paragraph.3.2.1.1.3}{DeepMDP agent: unable to balance multiple loss calculations}{subsubsection.3.2.1.1}% 28
\BOOKMARK [2][-]{subsection.3.2.2}{Research results: OpenAI Pong}{section.3.2}% 29
\BOOKMARK [3][-]{subsubsection.3.2.2.1}{Discussion}{subsection.3.2.2}% 30
\BOOKMARK [4][-]{paragraph.3.2.2.1.1}{PCA agent: losing spatial information}{subsubsection.3.2.2.1}% 31
\BOOKMARK [4][-]{paragraph.3.2.2.1.2}{Pre-trained autoencoder agent: slightly worse than the baseline agent}{subsubsection.3.2.2.1}% 32
\BOOKMARK [4][-]{paragraph.3.2.2.1.3}{Online trained autoencoder agent:}{subsubsection.3.2.2.1}% 33
\BOOKMARK [2][-]{subsection.3.2.3}{Results discussion}{section.3.2}% 34
\BOOKMARK [0][-]{chapter.4}{Related work}{}% 35
\BOOKMARK [0][-]{chapter.5}{Conclusions and future research}{}% 36
\BOOKMARK [0][-]{appendix.A}{Appendix}{}% 37
\BOOKMARK [1][-]{section.A.1}{Starcraft II: RL agent architectures}{appendix.A}% 38
\BOOKMARK [2][-]{subsection.A.1.1}{Baseline agent}{section.A.1}% 39
\BOOKMARK [2][-]{subsection.A.1.2}{PCA agent}{section.A.1}% 40
\BOOKMARK [2][-]{subsection.A.1.3}{Pre-trained and online trained autoencoder agent}{section.A.1}% 41
\BOOKMARK [2][-]{subsection.A.1.4}{DeepMDP agent}{section.A.1}% 42
\BOOKMARK [1][-]{section.A.2}{OpenAI Pong: RL agent architectures}{appendix.A}% 43
\BOOKMARK [2][-]{subsection.A.2.1}{Baseline agent}{section.A.2}% 44
\BOOKMARK [2][-]{subsection.A.2.2}{PCA agent}{section.A.2}% 45
\BOOKMARK [2][-]{subsection.A.2.3}{Pre-trained and online trained autoencoder agent}{section.A.2}% 46
