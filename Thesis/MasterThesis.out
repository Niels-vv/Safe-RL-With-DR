\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Problem statement}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Research questions}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Method, results and possible benefits}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{Paper structure}{chapter.1}% 5
\BOOKMARK [0][-]{chapter.2}{Related work}{}% 6
\BOOKMARK [0][-]{chapter.3}{Preliminaries}{}% 7
\BOOKMARK [1][-]{section.3.1}{Reinforcement learning}{chapter.3}% 8
\BOOKMARK [2][-]{subsection.3.1.1}{General overview}{section.3.1}% 9
\BOOKMARK [2][-]{subsection.3.1.2}{Artificial neural networks}{section.3.1}% 10
\BOOKMARK [2][-]{subsection.3.1.3}{Double deep-q-network}{section.3.1}% 11
\BOOKMARK [1][-]{section.3.2}{State-space dimensionality reduction}{chapter.3}% 12
\BOOKMARK [2][-]{subsection.3.2.1}{Principal Component Analysis}{section.3.2}% 13
\BOOKMARK [2][-]{subsection.3.2.2}{Autoencoder}{section.3.2}% 14
\BOOKMARK [2][-]{subsection.3.2.3}{DeepMDP}{section.3.2}% 15
\BOOKMARK [0][-]{chapter.4}{Research}{}% 16
\BOOKMARK [1][-]{section.4.1}{Methodology}{chapter.4}% 17
\BOOKMARK [2][-]{subsection.4.1.1}{Experiments}{section.4.1}% 18
\BOOKMARK [2][-]{subsection.4.1.2}{Environment: Starcraft II}{section.4.1}% 19
\BOOKMARK [2][-]{subsection.4.1.3}{Environment: OpenAI Pong}{section.4.1}% 20
\BOOKMARK [2][-]{subsection.4.1.4}{Remarks on method}{section.4.1}% 21
\BOOKMARK [1][-]{section.4.2}{Results}{chapter.4}% 22
\BOOKMARK [2][-]{subsection.4.2.1}{Research results: Starcraft II}{section.4.2}% 23
\BOOKMARK [3][-]{subsubsection.4.2.1.1}{Results overview}{subsection.4.2.1}% 24
\BOOKMARK [3][-]{subsubsection.4.2.1.2}{Discussion}{subsection.4.2.1}% 25
\BOOKMARK [4][-]{paragraph.4.2.1.2.1}{PCA Agent: losing all spatial information}{subsubsection.4.2.1.2}% 26
\BOOKMARK [4][-]{paragraph.4.2.1.2.2}{Autoencoder agents analyses: outperforming baseline agent}{subsubsection.4.2.1.2}% 27
\BOOKMARK [4][-]{paragraph.4.2.1.2.3}{DeepMDP agent: unable to balance multiple loss calculations}{subsubsection.4.2.1.2}% 28
\BOOKMARK [2][-]{subsection.4.2.2}{Research results: OpenAI Pong}{section.4.2}% 29
\BOOKMARK [3][-]{subsubsection.4.2.2.1}{Results overview}{subsection.4.2.2}% 30
\BOOKMARK [3][-]{subsubsection.4.2.2.2}{Discussion}{subsection.4.2.2}% 31
\BOOKMARK [4][-]{paragraph.4.2.2.2.1}{PCA agent: losing spatial information}{subsubsection.4.2.2.2}% 32
\BOOKMARK [4][-]{paragraph.4.2.2.2.2}{Pre-trained autoencoder agent: slightly worse than the baseline agent}{subsubsection.4.2.2.2}% 33
\BOOKMARK [4][-]{paragraph.4.2.2.2.3}{Online trained autoencoder agent: equaling the baseline agent's policy}{subsubsection.4.2.2.2}% 34
\BOOKMARK [2][-]{subsection.4.2.3}{Results discussion}{section.4.2}% 35
\BOOKMARK [0][-]{chapter.5}{Conclusions and future research}{}% 36
\BOOKMARK [0][-]{appendix.A}{Appendix}{}% 37
\BOOKMARK [1][-]{section.A.1}{Starcraft II: RL agent architectures}{appendix.A}% 38
\BOOKMARK [2][-]{subsection.A.1.1}{Baseline agent}{section.A.1}% 39
\BOOKMARK [2][-]{subsection.A.1.2}{PCA agent}{section.A.1}% 40
\BOOKMARK [2][-]{subsection.A.1.3}{Pre-trained and online trained autoencoder agent}{section.A.1}% 41
\BOOKMARK [2][-]{subsection.A.1.4}{DeepMDP agent}{section.A.1}% 42
\BOOKMARK [1][-]{section.A.2}{OpenAI Pong: RL agent architectures}{appendix.A}% 43
\BOOKMARK [2][-]{subsection.A.2.1}{Baseline agent}{section.A.2}% 44
\BOOKMARK [2][-]{subsection.A.2.2}{PCA agent}{section.A.2}% 45
\BOOKMARK [2][-]{subsection.A.2.3}{Pre-trained and online trained autoencoder agent}{section.A.2}% 46
