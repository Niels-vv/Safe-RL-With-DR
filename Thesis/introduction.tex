\chapter{Introduction}\label{introduction}
\section{Problem statement}
With the advance of \emph{deep reinforcement learning} in $2013$, \emph{intelligent agents} have increasingly been able to solve more complex problems \cite{deeprl}. In reinforcement learning (RL), an agent is trained to solve a problem through trial and error, using feedback in the form of rewards. This way it tries to find an optimal \emph{policy}: a mapping of a state and its available actions to a valuation for each action. For \emph{deep} RL, \emph{artificial neural networks} are used to train such an agent. The usage of increasingly complex problem settings, comes with more complex environments for an agent to work in; observations representing the current state of the environment are becoming dimensionally higher.

This means that the \emph{state-space} of the environment, the number of possible states the environment can be in, is increasing. In general, the larger the state-space, the more difficult and time intensive the training of the agent will be, and thus the more computing cost is needed. This is due to several reasons. Firstly, with a larger state-space, the agent will need to explore more states before having explored the entire state-space. This leads to having to gather larger datasets of state observations which can be impractical \cite{AE_2019}.

Secondly, for deep RL, the agent uses artificial neural networks. These networks need to be of a size proportional to the state-space to be able to accurately approximate an optimal policy; otherwise it could lead to under-fitting or over-fitting \cite{rlfitting}. Generally, the larger the state-space, the larger such a network needs to be. This means the network will contain more trainable parameters, leading to more computation for its training and its usage \cite{AE_2019}.

Lastly, higher dimensional states may include more irrelevant information and noise. This can lead to longer training times for an agent and even to agents not being able to find good policies \cite{AE_2016}.

\section{Research questions}
A way for dealing with large state-spaces is needed for RL to be scalable. One way to deal with this, is by lowering the state-space of a specific problem, whilst retaining enough state information for the agent to find an optimal policy. This is known as \emph{state-space dimensionality reduction}. Here, a state observation would be projected to a lower dimensional space; this lower dimensional representation is called the \emph{latent representation}. The main problem with state-space dimensionality reduction, is the loss of possibly essential information, which could lead to RL agents not being able to find an optimal policy. 

In this thesis, we will examine the effect of using state-space dimensionality reduction on an RL agent, focusing on whether an agent can find optimal policies on a lower dimensional state-space. More specifically, we will look at the effect on a model-free, value based RL agent (as explained in section \ref{pl-rl}). For this, we will use the \emph{double deep-Q-learning} (DDQN) learning algorithm (see section \ref{pl-dqn}) \cite{ddqn}. We will project the state observation to a lower dimensional space using three different methods, whose effects on the RL agent we will compare: \emph{principal component analysis}, \emph{autoencoders} and \emph{DeepMDPs} (see sections \ref{pl-pca}, \ref{pl-ae} and \ref{pl-deepmdp} respectively for information about each method). We will apply these methods on two environments that use high dimensional grid-based observations: Starcraft II MoveToBeacon (see \ref{research-env-pysc2}) and OpenAI Atari Pong (see \ref{research-env-pong}) \cite{pysc2}\cite{pong}.

The main research question we will examine is: \textit{what is the effect of state-space dimensionality reduction on model-free value-based reinforcement learning in an environment using grid-based observations, by using PCA, autoencoders or DeepMDPs}? To answer this, we will answer the research sub-question: \textit{how do the training results of a double deep-Q-learning reinforcement learning agent change when using PCA, autoencoders and DeepMDPs for state-space dimensionality reduction in Starcraft II MoveToBeacon and OpenAI Atari Pong}?

\section{Method, results and possible benefits}
We gathered the training results for several RL agents trained in Starcraft II and Pong. The first agent is a baseline agent: a DDQN RL agent using the observations given by the environment. The second agent uses PCA to reduce the dimensionality of the observations. Thus, the observations given by the environment first go through PCA, thereby projected to a lower dimensional space, before being used by the RL agent. The PCA is trained on previously stored observation traces, before being used by the RL agent. There are also two agents using an autoencoder, that work similar to the PCA agent. The first of the two uses a pre-trained autoencoder, again trained on stored observations. The second autoencoder agent, the online trained autoencoder agent, is trained simultaneously with the RL agent, using the observations received by the agent. Lastly, there is a DeepMDP agent, which is only used in the Starcraft II environment.

In both environments, the PCA agent lost all spatial information, leading to RL agents learning sub-optimal policies. The DeepMDP was also unable to train to an optimal policy in Starcraft II. However, the autoencoder agents were able to learn good policies. In the Starcraft II environment, the pre-trained autoencoder agent learned a better, more stable policy in less episodes than the baseline agent. The online trained autoencoder agent had training results similar to the baseline agent, yet slightly more inconsistent, and needing more episodes to train well. Similar results were found for the second environment, Pong. However, the pre-trained autoencoder agent learned a slightly worse and less consistent policy than the baseline agent. This resulted from the agent needing more precise spatial information than in the Starcraft II environment. However, the online trained autoencoder agent reached a policy similar to the baseline agent, though needing more episodes to converge. Its better converging policy can be explained by the autoencoder being trained on more frames than the pre-trained autoencoder, therewith giving a more precise latent representation. However, the precision needed in the spatial information for the RL to find an optimal policy, show a limit with regards to the usage of an autoencoder agent.

Several benefits may arise from training agents on lower state-spaces. Though we will mention these benefits now here, the extend of these benefits are not examined in this thesis and are left for future research.

Firstly, RL agents training in a lowered state-space might need less training time and less observation data, and therefore less computational cost for training and using its neural network. 

Secondly, in very complex, large scaled environments, a normal RL agent possibly cannot find an optimal policy in a feasible amount of time; here, state-space dimensionality reduction could to lead to a better policy. 

Thirdly, depending on the dimensionality reduction method, we could reuse the dimensionality reduction component in different RL agents solving different problems in the same environment. Thus, we would only have to train a dimensionality reduction component once, then being able to train different agents on the same latent representation. This possibility though depends on two things. It would need to be a dimensionality reduction method that is trainable separately from the RL agent. This is the case for only two of the three methods we consider in our research: PCA and autoencoders. Secondly, the latent representation would have to be problem-independent: it would have to be a good representation of the entire state-space, without losing information essential to any of the RL problems being used in this environment.

\section{Paper structure}
We will start by giving preliminary information in section \ref{preliminaries}. Here we will first give an overview of reinforcement learning principles in section \ref{pl-rl}. This will include an explanation of artificial neural networks and the RL learning algorithm DDQN, which we use for our agents. After this, we will discuss state-space dimensionality reduction in section \ref{pl-dimensionality}, which will include an examination of the three methods used in our research: PCA, autoencoders and DeepMDPs.

After the preliminaries, we will show our methodology in section \ref{research-method}. Here we will give a general overview of the different RL agents that we will use for our experiments. Then we will give information about the environments used and the specific agent setups per environment. 

After having explained the experiment setups, we will give the results of our experiments in section \ref{research-results}. We will show and discuss the results per environment, before discussing the results of both environments as a whole.

Then, we will discuss related work in section \ref{relatedwork}, before giving our conclusions and suggestions for future research in section \ref{conclusions}. Lastly, the appendices in section \ref{appendix} will contain more details on the specific agent architectures per environment, including hyperparameter settings.

Our code repository can be found at: \url{https://github.com/Niels-vv/Safe-RL-With-DR.git}.