500 eps training
Tried setups:

DQN, 800 episodes, evaluation eposode every 15th episode
	1. Conv, screen size 32 ipv 84: random
	Seed 0 ipv 3, deterministic en benchmark weggecomment
	
			 'train_q_per_step' : 10,
                        'gamma' : 0.99,
                        'train_q_batch_size' : 256,
                        'batches_before_training' : 10,
                        'target_q_update_frequency' : 500,
                        'lr' : 0.00005,
                        'plot_every' : 10,
                        'decay_steps' : 100000
                        
         2.		train_q_per_step : 5, target_q_update_frequence: 250
         
         3.		train_q_per_setp : 4, 'batches_before_training' : 20,'target_q_update_frequency' : 500, 'lr' : 0.001, 
         
         4.		train_q_per_setp : 4, 'batches_before_training' : 20,'target_q_update_frequency' : 250, 'lr' : 0.0001, 
         
         4 Geeft beste resultaat
         
         5.		Zoals 4 maar met flatten obs naar linear layer voor conv layers (tussendoor reshape(x.shape[0]),1,32,32)
         
         
         Failed:
         -		train_q_per_setp : 4, 'batches_before_training' : 20,'target_q_update_frequency' : 1000, 'lr' : 0.01, 
         		
         		Te hoge lr wsl, leerde even en zakte toen snel omlaag
         		
         -		train_q_per_setp : 4, 'batches_before_training' : 20,'target_q_update_frequency' : 250, 'lr' : 0.00001, decay_steps: 200000 
         
         - 		Linear: self.hidden_dims = [input_size*2, input_size*10,input_size*2]
         
         te lage lr?
         
         
